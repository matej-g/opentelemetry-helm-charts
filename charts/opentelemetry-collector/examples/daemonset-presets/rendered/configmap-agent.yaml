---
# Source: opentelemetry-collector/templates/configmap-agent.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-opentelemetry-collector-agent
  namespace: default
  labels:
<<<<<<< Updated upstream
    helm.sh/chart: opentelemetry-collector-0.87.3
=======
    helm.sh/chart: opentelemetry-collector-0.87.4
>>>>>>> Stashed changes
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: example
    app.kubernetes.io/version: "0.106.0"
    app.kubernetes.io/managed-by: Helm
    
data:
  relay: |
    connectors:
      spanmetrics:
        histogram:
          explicit:
            buckets:
            - 1ms
            - 1s
            - 10s
        metrics_expiration: 5m
        metrics_flush_interval: 30s
    exporters:
      debug: {}
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
      k8s_observer:
        auth_type: serviceAccount
        node: ${env:K8S_NODE_NAME}
        observe_pods: true
    processors:
      batch: {}
      k8sattributes:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.replicaset.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
      transform/k8s_attributes:
        log_statements:
        - context: resource
          statements:
          - set(attributes["k8s.deployment.name"], attributes["k8s.replicaset.name"])
          - replace_pattern(attributes["k8s.deployment.name"], "^(.*)-[0-9a-zA-Z]+$",
            "$$1") where attributes["k8s.replicaset.name"] != nil
          - delete_key(attributes, "k8s.replicaset.name")
        metric_statements:
        - context: resource
          statements:
          - set(attributes["k8s.deployment.name"], attributes["k8s.replicaset.name"])
          - replace_pattern(attributes["k8s.deployment.name"], "^(.*)-[0-9a-zA-Z]+$",
            "$$1") where attributes["k8s.replicaset.name"] != nil
          - delete_key(attributes, "k8s.replicaset.name")
        trace_statements:
        - context: resource
          statements:
          - set(attributes["k8s.deployment.name"], attributes["k8s.replicaset.name"])
          - replace_pattern(attributes["k8s.deployment.name"], "^(.*)-[0-9a-zA-Z]+$",
            "$$1") where attributes["k8s.replicaset.name"] != nil
          - delete_key(attributes, "k8s.replicaset.name")
      transform/reduce:
        error_mode: ignore
        metric_statements:
        - context: resource
          statements:
          - delete_key(attributes, "container.id")
          - delete_key(attributes, "k8s.pod.uid")
          - delete_key(attributes, "k8s.replicaset.uid")
          - delete_key(attributes, "k8s.daemonset.uid")
          - delete_key(attributes, "k8s.deployment.uid")
          - delete_key(attributes, "k8s.statefulset.uid")
          - delete_key(attributes, "k8s.cronjob.uid")
          - delete_key(attributes, "k8s.job.uid")
          - delete_key(attributes, "k8s.hpa.uid")
          - delete_key(attributes, "k8s.namespace.uid")
          - delete_key(attributes, "k8s.node.uid")
          - delete_key(attributes, "net.host.name")
          - delete_key(attributes, "net.host.port")
      transform/span_name:
        trace_statements:
        - context: span
          statements:
          - replace_pattern(name, "^(.*)$", "$$1")
    receivers:
      filelog:
        exclude: []
        force_flush_period: 0
        include:
        - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        operators:
        - id: get-format
          routes:
          - expr: body matches "^\\{"
            output: parser-docker
          - expr: body matches "^[^ Z]+ "
            output: parser-crio
          - expr: body matches "^[^ Z]+Z"
            output: parser-containerd
          type: router
        - id: parser-crio
          regex: ^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
          timestamp:
            layout: 2006-01-02T15:04:05.999999999Z07:00
            layout_type: gotime
            parse_from: attributes.time
          type: regex_parser
        - combine_field: attributes.log
          combine_with: ""
          id: crio-recombine
          is_last_entry: attributes.logtag == 'F'
          max_log_size: 1048576
          output: handle_empty_log
          source_identifier: attributes["log.file.path"]
          type: recombine
        - id: parser-containerd
          regex: ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
          timestamp:
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            parse_from: attributes.time
          type: regex_parser
        - combine_field: attributes.log
          combine_with: ""
          id: containerd-recombine
          is_last_entry: attributes.logtag == 'F'
          max_log_size: 1048576
          output: handle_empty_log
          source_identifier: attributes["log.file.path"]
          type: recombine
        - id: parser-docker
          timestamp:
            layout: '%Y-%m-%dT%H:%M:%S.%LZ'
            parse_from: attributes.time
          type: json_parser
        - combine_field: attributes.log
          combine_with: ""
          id: docker-recombine
          is_last_entry: attributes.log endsWith "\n"
          max_log_size: 1048576
          output: handle_empty_log
          source_identifier: attributes["log.file.path"]
          type: recombine
        - field: attributes.log
          id: handle_empty_log
          if: attributes.log == nil
          type: add
          value: ""
        - parse_from: attributes["log.file.path"]
          regex: ^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$
          type: regex_parser
        - from: attributes.stream
          to: attributes["log.iostream"]
          type: move
        - from: attributes.container_name
          to: resource["k8s.container.name"]
          type: move
        - from: attributes.namespace
          to: resource["k8s.namespace.name"]
          type: move
        - from: attributes.pod_name
          to: resource["k8s.pod.name"]
          type: move
        - from: attributes.restart_count
          to: resource["k8s.container.restart_count"]
          type: move
        - from: attributes.uid
          to: resource["k8s.pod.uid"]
          type: move
        - default: clean-up-log-record
          routes:
          - expr: (resource["k8s.namespace.name"]) == "kube-system" && (resource["k8s.pod.name"])
              matches "app-.*" && (resource["k8s.container.name"]) == "http"
            output: kube-system_app-.*_http
          - expr: (resource["k8s.namespace.name"]) == "default" && (resource["k8s.pod.name"])
              matches "app-.*"
            output: default_app-.*
          - expr: (resource["k8s.namespace.name"]) == "test"
            output: test
          type: router
        - combine_field: attributes.log
          combine_with: ""
          id: kube-system_app-.*_http
          is_first_entry: (attributes.log) matches "^[^\\s].*"
          max_log_size: 1048576
          max_unmatched_batch_size: 1
          output: clean-up-log-record
          type: recombine
        - combine_field: attributes.log
          combine_with: ""
          id: default_app-.*
          is_first_entry: (attributes.log) matches "^[^\\s].*"
          max_log_size: 1048576
          max_unmatched_batch_size: 1
          output: clean-up-log-record
          type: recombine
        - combine_field: attributes.log
          combine_with: ""
          id: test
          is_first_entry: (attributes.log) matches "^[^\\s].*"
          max_log_size: 1048576
          max_unmatched_batch_size: 1
          output: clean-up-log-record
          type: recombine
        - from: attributes.log
          id: clean-up-log-record
          to: body
          type: move
        - drop_ratio: 1
          expr: (attributes["log.file.path"] matches "/var/log/pods/default_example-opentelemetry-collector.*_.*/opentelemetry-collector/.*.log")
            and ((body contains "logRecord") or (body contains "ResourceLog"))
          type: filter
        - combine_field: body
          is_first_entry: body matches "^([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}|[0-9]{2}-[0-9]{2}
            [0-9]{2}:[0-9]{2}:[0-9]{2})"
          source_identifier: attributes["log.file.path"]
          type: recombine
        retry_on_failure:
          enabled: true
        start_at: beginning
      jaeger:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:14250
          thrift_compact:
            endpoint: ${env:MY_POD_IP}:6831
          thrift_http:
            endpoint: ${env:MY_POD_IP}:14268
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - ${env:MY_POD_IP}:8888
      receiver_creator/mysql:
        receivers:
          mysql:
            config:
              collection_interval: 30s
              metrics:
                mysql.client.network.io:
                  enabled: true
                mysql.commands:
                  enabled: true
                mysql.connection.errors:
                  enabled: true
                mysql.joins:
                  enabled: true
                mysql.query.count:
                  enabled: true
                mysql.query.slow.count:
                  enabled: true
                mysql.sorts:
                  enabled: true
                mysql.table_open_cache:
                  enabled: true
              password: ${env:MYSQL_PASSWORD}
              statement_events:
                digest_text_limit: 120
                limit: 250
                time_limit: 24h
              username: root
            rule: type == "port" && port == 3306 && pod.labels["env"] == "test" && pod.labels["team"]
              == "teamA"
        watch_observers:
        - k8s_observer
      zipkin:
        endpoint: ${env:MY_POD_IP}:9411
    service:
      extensions:
      - health_check
      - k8s_observer
      pipelines:
        logs:
          exporters:
          - debug
          processors:
          - k8sattributes
          - memory_limiter
          - transform/k8s_attributes
          - batch
          receivers:
          - otlp
          - filelog
        metrics:
          exporters:
          - debug
          processors:
          - k8sattributes
          - memory_limiter
          - transform/k8s_attributes
          - transform/reduce
          - batch
          receivers:
          - otlp
          - prometheus
          - receiver_creator/mysql
          - spanmetrics
        traces:
          exporters:
          - debug
          - spanmetrics
          processors:
          - transform/span_name
          - k8sattributes
          - memory_limiter
          - transform/k8s_attributes
          - batch
          receivers:
          - otlp
          - jaeger
          - zipkin
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
